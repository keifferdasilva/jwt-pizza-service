# Incident: 2024-12-05 11-49-03

## Summary

Write a summary of the incident in a few sentences. Include what happened, why, the severity of the incident and how long the impact lasted.

Between the hours of 11:49 and 14:42 on December 5th, pizzas were not being created and pizzas could not be sold. The event was triggered by an unknown fault at the pizza factory. 

Unfortunately, the error was not detected by the alerts that were in place, however the team noticed the issue during a manual review of the metrics. The team started working on the event by looking at the logs and finding that the factory was down. Following the link that was returned by the factory resolved the issue. This major incident affected all users.

## Detection

When did the team detect the incident? How did they know it was happening? How could we improve time-to-detection? Consider: How would we have cut that time by half?

This incident was detected when Keiffer looked at the metrics and logs and realized that there was no data for pizza creation latency or pizza purchases as well as an uptick in error logs.

The time to detect the incident could have been improved by alerting if no data was being reported for the two metrics mentioned abovve and lowering the threshold for the alert related to error logs.

Changing the alert rules would improve the detection time significantly if a similar problem should occur in the future.

## Impact

Describe how the incident impacted internal and external users during the incident. Include how many support cases were raised.

For 2hrs 46 minutes between 11:49 MST and 14:45 MST on 12/05/2024, an issue at the pizza factory impacted pizza ordering.

This incident affected 100% of users who could not order pizzas.

## Timeline

Detail the incident timeline. We recommend using UTC to standardize for timezones.

Include any notable lead-up events, any starts of activity, the first known impact, and escalations. Note any decisions or changed made, and when the incident ended, along with any post-impact events of note.


All times are MST.

- _11:49_ - Pizza factory went down
- _14:40_ - Keiffer looked at the dashboard
- _14:44_ - Keiffer went to the link that was returned by the pizza factory
- _14:45_ - Functionality seemed to be restored as pizza sold and creation metrics started coming through again
- _14:50_ - Keiffer looked at logs in AWS
- _14:53_ - Keiffer looked at logs in Grafana
- _15:15_ - Keiffer tried debugging on the dev environment to try and isolate the issue.
- _16:38_ - Keiffer contacted Stephen and realized that clicking the link resolved the issue.

## Response

Who responded to the incident? When did they respond, and what did they do? Note any delays or obstacles to responding.


Keiffer looked at the dashboard at 14:40 MST and realized there was a problem. The delay in response was due to faulty alerts related to the pizza factory metrics. They looked at the logs and went to the link that was returned by the factory when trying to create pizzas. 

They continued to look at the logs in both Grafana and AWS to try and isolate the issue. They then tried to debug in the dev environment to see if the problem was in the server code.

# Root cause

Note the final root cause of the incident, the thing identified that needs to change in order to prevent this class of incident from happening again.

The pizza factory went down. Better metrics to alert when the factory is down and communication with the pizza factory could prevent this issue from having a great impact.

## Resolution

Describe how the service was restored and the incident was deemed over. Detail how the service was successfully restored and you knew how what steps you needed to take to recovery.

Depending on the scenario, consider these questions: How could you improve time to mitigation? How could you have cut that time by half?

The service was restored by clicking on the link returned by the pizza factory and reestablishing the connection. It was not clear at first what was needed but looking at the logs led us to the solution. Time to mitigation could have been improved by setting up better alerts.

# Prevention

Now that you know the root cause, can you look back and see any other incidents that could have the same root cause? If yes, note what mitigation was attempted in those incidents and ask why this incident occurred again.

There were no other incidents that had the same root cause.

# Action items

Describe the corrective action ordered to prevent this class of incident in the future. Note who is responsible and when they have to complete the work and where that work is being tracked.


1. Alerts should be adjusted to correctly diagnose any similar issues with the factory not working.
2. Test the service with the incorrect factory to make sure that the alerts are working.
